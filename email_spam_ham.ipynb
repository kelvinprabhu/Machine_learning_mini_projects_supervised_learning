{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\e'\n",
      "C:\\Users\\kelvi\\AppData\\Local\\Temp\\ipykernel_12508\\2837076422.py:5: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  df = pd.read_csv('datasets\\email_spam.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84 entries, 0 to 83\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   84 non-null     object\n",
      " 1   text    84 non-null     object\n",
      " 2   type    84 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                               title  \\\n",
       " 0                          ?? the secrets to SUCCESS   \n",
       " 1                    ?? You Earned 500 GCLoot Points   \n",
       " 2                         ?? Your GitHub launch code   \n",
       " 3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       " 4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       " \n",
       "                                                 text      type  \n",
       " 0  Hi James,\\n\\nHave you claim your complimentary...      spam  \n",
       " 1  \\nalt_text\\nCongratulations, you just earned\\n...  not spam  \n",
       " 2  Here's your GitHub launch code, @Mortyj420!\\n ...  not spam  \n",
       " 3  Hello,\\n \\nThank you for contacting the Virtua...  not spam  \n",
       " 4  Hey Prachanda Rawal,\\n\\nToday's newsletter is ...      spam  ,\n",
       " None)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df = pd.read_csv('datasets\\email_spam.csv')\n",
    "df.head(),df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 3)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df.copy()\n",
    "df1.duplicated().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\n\\nHave you claim your complimentary...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\nalt_text\\nCongratulations, you just earned\\n...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\n ...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\n \\nThank you for contacting the Virtua...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\n\\nToday's newsletter is ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \n",
       "0  Hi James,\\n\\nHave you claim your complimentary...      spam  \n",
       "1  \\nalt_text\\nCongratulations, you just earned\\n...  not spam  \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\n ...  not spam  \n",
       "3  Hello,\\n \\nThank you for contacting the Virtua...  not spam  \n",
       "4  Hey Prachanda Rawal,\\n\\nToday's newsletter is ...      spam  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"spam\"] = df1[\"type\"].apply(lambda x: 1 if x == \"spam\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi James,Have you claim your complimentary gif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alt_textCongratulations, you just earned500You...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420! an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello, Thank you for contacting the Virtual Re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hey Prachanda Rawal,Today's newsletter is Jam-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Dear Maryam,  I would like to thank you for yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Dear Customer,Welcome to Kilimall, Thanks so m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Dear vladis163rus,Here is the Steam Guard code...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>View In Browser | Log in  Skrill logoMoney mov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>You've received a gift!Sign in to your Bard Ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  spam\n",
       "0   Hi James,Have you claim your complimentary gif...     1\n",
       "1   alt_textCongratulations, you just earned500You...     0\n",
       "2   Here's your GitHub launch code, @Mortyj420! an...     0\n",
       "3   Hello, Thank you for contacting the Virtual Re...     0\n",
       "4   Hey Prachanda Rawal,Today's newsletter is Jam-...     1\n",
       "..                                                ...   ...\n",
       "79  Dear Maryam,  I would like to thank you for yo...     0\n",
       "80  Dear Customer,Welcome to Kilimall, Thanks so m...     0\n",
       "81  Dear vladis163rus,Here is the Steam Guard code...     0\n",
       "82  View In Browser | Log in  Skrill logoMoney mov...     0\n",
       "83  You've received a gift!Sign in to your Bard Ex...     0\n",
       "\n",
       "[84 rows x 2 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"text\"] = df1['text'].replace('\\n','',regex=True)\n",
    "df1[[\"text\",\"spam\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (3.8.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from wordcloud) (10.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from wordcloud) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "%pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kelvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kelvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kelvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kelvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import wordcloud\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,Have you claim your complimentary gif...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>alt_textCongratulations, you just earned500You...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "0        ?? the secrets to SUCCESS   \n",
       "1  ?? You Earned 500 GCLoot Points   \n",
       "\n",
       "                                                text      type  spam  \n",
       "0  Hi James,Have you claim your complimentary gif...      spam     1  \n",
       "1  alt_textCongratulations, you just earned500You...  not spam     0  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Hi James,\\n\\nHave you claim your complimentary...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform replacements and clean the text\n",
    "# df1['text'] = df1['text'].str.replace(r'^.+@[^\\.*].[a-z]{2,}$', 'emailaddress', regex=True)\n",
    "# df1['text'] = df1['text'].str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)', 'webaddress', regex=True)\n",
    "# df1['text'] = df1['text'].str.replace(r'€|\\$', 'dollars', regex=True)\n",
    "# df1['text'] = df1['text'].str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$', 'phonenumber', regex=True)\n",
    "# df1['text'] = df1['text'].str.replace(r'\\d+(\\.\\d+)?', 'number', regex=True)\n",
    "# df1['text'] = df1['text'].str.replace(r'[^\\w\\d\\s]', '', regex=True)  # Remove punctuation\n",
    "# df1['text'] = df1['text'].str.replace(r'_', '', regex=True)  # Remove underscores\n",
    "\n",
    "# # Remove multiple spaces and leading/trailing spaces\n",
    "# df1['text'] = df1['text'].str.strip()\n",
    "# df1['text'] = df1['text'].str.replace(r'\\s+', ' ', regex=True)  # Replace multiple spaces with a single space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement string (from versions: none)\n",
      "ERROR: No matching distribution found for string\n"
     ]
    }
   ],
   "source": [
    "%pip install string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    " \n",
    "#storing the punctuation free text for both training and testing data\n",
    "# df['text']= df['text'].apply(lambda x:remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# \", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement re (from versions: none)\n",
      "ERROR: No matching distribution found for re\n"
     ]
    }
   ],
   "source": [
    "%pip install re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1['cleaned_tokens']= df1['text'].apply(lambda x:remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1['cleaned_tokens'].head(),df1['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "# # Download stopwords if not already done\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# # Load stopwords\n",
    "# stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# # Define tokenization function (tokenizing by words, not letters)\n",
    "# def tokenization(text):\n",
    "#     tokenizer = WhitespaceTokenizer()  # Tokenizes based on whitespace\n",
    "#     tokens = tokenizer.tokenize(text)  # Tokenizes the text into words\n",
    "#     return tokens\n",
    "\n",
    "# # Define stopword removal function\n",
    "# def remove_stopwords(text_tokens):\n",
    "#     output = [word for word in text_tokens if word.lower() not in stopwords]  # Remove stopwords\n",
    "#     return output\n",
    "\n",
    "# # Apply tokenization and stopword removal\n",
    "# df1['tokenized_text'] = df1['text'].apply(tokenization)  # Tokenize by words\n",
    "# df1['cleaned_text'] = df1['tokenized_text'].apply(remove_stopwords)  # Remove stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Hi James,Have you claim your complimentary gif...   \n",
      "1  alt_textCongratulations, you just earned500You...   \n",
      "2  Here's your GitHub launch code, @Mortyj420! an...   \n",
      "3  Hello, Thank you for contacting the Virtual Re...   \n",
      "4  Hey Prachanda Rawal,Today's newsletter is Jam-...   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  Hi JamesHave you claim your complimentary gift...   \n",
      "1  alttextCongratulations you just earnednumberYo...   \n",
      "2  Heres your GitHub launch code Mortyjnumber an ...   \n",
      "3  Hello Thank you for contacting the Virtual Rew...   \n",
      "4  Hey Prachanda RawalTodays newsletter is JamPac...   \n",
      "\n",
      "                                        final_tokens  \n",
      "0  [Hi, JamesHave, claim, complimentary, gift, ye...  \n",
      "1  [alttextCongratulations, earnednumberYou, comp...  \n",
      "2  [Heres, GitHub, launch, code, Mortyjnumber, oc...  \n",
      "3  [Hello, Thank, contacting, Virtual, Reward, Ce...  \n",
      "4  [Hey, Prachanda, RawalTodays, newsletter, JamP...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kelvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Define text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'^.+@[^\\.*].[a-z]{2,}$', 'emailaddress', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?', 'webaddress', text)\n",
    "    # Replace currency symbols\n",
    "    text = re.sub(r'€|\\$', 'dollars', text)\n",
    "    # Replace phone numbers\n",
    "    text = re.sub(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$', 'phonenumber', text)\n",
    "    # Replace numbers with \"number\"\n",
    "    text = re.sub(r'\\d+(\\.\\d+)?', 'number', text)\n",
    "    # Remove punctuation (non-alphanumeric characters)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove underscores\n",
    "    text = re.sub(r'_', '', text)\n",
    "    # Remove extra spaces and trim leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize(text):\n",
    "    tokenizer = WhitespaceTokenizer()  # Tokenizes based on spaces\n",
    "    tokens = tokenizer.tokenize(text)  # Tokenizes into words\n",
    "    return tokens\n",
    "\n",
    "# Define stopword removal function\n",
    "def remove_stopwords(tokens):\n",
    "    cleaned_tokens = [word for word in tokens if word.lower() not in stopwords]  # Remove stopwords\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Example: Apply the process to a dataframe column\n",
    "def process_text_column(df, text_column):\n",
    "    # Step 1: Clean the text\n",
    "    df['cleaned_text'] = df[text_column].apply(clean_text)\n",
    "    \n",
    "    # Step 2: Tokenize the cleaned text\n",
    "    df['tokenized_text'] = df['cleaned_text'].apply(tokenize)\n",
    "    \n",
    "    # Step 3: Remove stopwords\n",
    "    df['final_tokens'] = df['tokenized_text'].apply(remove_stopwords)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Assuming df1 is your dataframe and 'text' is the column with raw text\n",
    "df1 = process_text_column(df1, 'text')\n",
    "\n",
    "# To check results\n",
    "print(df1[['text', 'cleaned_text', 'final_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('number', 167),\n",
       " ('email', 49),\n",
       " ('account', 34),\n",
       " ('numbernumber', 29),\n",
       " ('please', 27),\n",
       " ('us', 23),\n",
       " ('like', 18),\n",
       " ('information', 18),\n",
       " ('address', 18),\n",
       " ('may', 17)]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df1[\"final_tokens\"].values:\n",
    "    for word in text:\n",
    "        cnt[word] += 1\n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df1[\"final_tokens\"].values:\n",
    "    for word in text:\n",
    "        cnt[word] += 1\n",
    " \n",
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    " \n",
    "def remove_freqwords(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in text if word not in FREQWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the Stemming function from nltk library\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#defining the object for stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    " \n",
    "#defining a function for stemming\n",
    "def stemming(text):\n",
    "  stem_text = [porter_stemmer.stem(word) for word in text]\n",
    "  return stem_text\n",
    " \n",
    "# applying function for stemming\n",
    "df1['final_tokens']=df1['final_tokens'].apply(lambda x: stemming(x))\n",
    "df1['final_tokens']=df1['final_tokens'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spellchecker\n",
      "  Downloading spellchecker-0.4.tar.gz (3.9 MB)\n",
      "     ---------------------------------------- 0.0/3.9 MB ? eta -:--:--\n",
      "     --------------------- ------------------ 2.1/3.9 MB 11.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 3.7/3.9 MB 9.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 3.7/3.9 MB 9.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 3.7/3.9 MB 9.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.9/3.9 MB 4.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: setuptools in c:\\users\\kelvi\\anaconda3\\lib\\site-packages (from spellchecker) (69.5.1)\n",
      "Collecting inexactsearch (from spellchecker)\n",
      "  Downloading inexactsearch-1.0.2.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting soundex>=1.0 (from inexactsearch->spellchecker)\n",
      "  Downloading soundex-1.1.3.tar.gz (9.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting silpa_common>=0.3 (from inexactsearch->spellchecker)\n",
      "  Downloading silpa_common-0.3.tar.gz (9.4 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: spellchecker, inexactsearch, silpa_common, soundex\n",
      "  Building wheel for spellchecker (setup.py): started\n",
      "  Building wheel for spellchecker (setup.py): finished with status 'done'\n",
      "  Created wheel for spellchecker: filename=spellchecker-0.4-py3-none-any.whl size=3966511 sha256=76a45eed2674815ea9ed2813cc7a1f611011561dc337e95a293c897e56c810aa\n",
      "  Stored in directory: c:\\users\\kelvi\\appdata\\local\\pip\\cache\\wheels\\e3\\d3\\a8\\831ad780bcfd3c2f29f8c20b1af1b54c925684e0efbdb67c74\n",
      "  Building wheel for inexactsearch (setup.py): started\n",
      "  Building wheel for inexactsearch (setup.py): finished with status 'done'\n",
      "  Created wheel for inexactsearch: filename=inexactsearch-1.0.2-py3-none-any.whl size=7148 sha256=ce44ced9f3768158cf264bc6d4e04f95ab8c1f41813741444698e72c635797c8\n",
      "  Stored in directory: c:\\users\\kelvi\\appdata\\local\\pip\\cache\\wheels\\8e\\c8\\8e\\d600631e3ef6d178300f44071a43bcc5f06b1223707d53c736\n",
      "  Building wheel for silpa_common (setup.py): started\n",
      "  Building wheel for silpa_common (setup.py): finished with status 'done'\n",
      "  Created wheel for silpa_common: filename=silpa_common-0.3-py3-none-any.whl size=8486 sha256=c3eb743bca196dccc7c174dee712ceba7b434da4e63e05a7b1846da58fa4a4b6\n",
      "  Stored in directory: c:\\users\\kelvi\\appdata\\local\\pip\\cache\\wheels\\cf\\d2\\8d\\3e359d137f6114538f7b9c6b06c1855a12050de481790f839c\n",
      "  Building wheel for soundex (setup.py): started\n",
      "  Building wheel for soundex (setup.py): finished with status 'done'\n",
      "  Created wheel for soundex: filename=soundex-1.1.3-py3-none-any.whl size=8891 sha256=f0a5c86c7a24340eb32003f6b3478c51e56af976d677474d7af42196c0eeafb2\n",
      "  Stored in directory: c:\\users\\kelvi\\appdata\\local\\pip\\cache\\wheels\\f2\\3f\\e6\\7abb4f928e7684b9f65509ebcd241cad91f260e35d1b9c0bc2\n",
      "Successfully built spellchecker inexactsearch silpa_common soundex\n",
      "Installing collected packages: silpa_common, soundex, inexactsearch, spellchecker\n",
      "Successfully installed inexactsearch-1.0.2 silpa_common-0.3 soundex-1.1.3 spellchecker-0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>spam</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>final_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,Have you claim your complimentary gif...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>Hi JamesHave you claim your complimentary gift...</td>\n",
       "      <td>[Hi, JamesHave, you, claim, your, complimentar...</td>\n",
       "      <td>[hi, jameshav, claim, complimentari, gift, yet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>alt_textCongratulations, you just earned500You...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>0</td>\n",
       "      <td>alttextCongratulations you just earnednumberYo...</td>\n",
       "      <td>[alttextCongratulations, you, just, earnednumb...</td>\n",
       "      <td>[alttextcongratul, earnednumbery, complet, fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420! an...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>0</td>\n",
       "      <td>Heres your GitHub launch code Mortyjnumber an ...</td>\n",
       "      <td>[Heres, your, GitHub, launch, code, Mortyjnumb...</td>\n",
       "      <td>[here, github, launch, code, mortyjnumb, octoc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "0        ?? the secrets to SUCCESS   \n",
       "1  ?? You Earned 500 GCLoot Points   \n",
       "2       ?? Your GitHub launch code   \n",
       "\n",
       "                                                text      type  spam  \\\n",
       "0  Hi James,Have you claim your complimentary gif...      spam     1   \n",
       "1  alt_textCongratulations, you just earned500You...  not spam     0   \n",
       "2  Here's your GitHub launch code, @Mortyj420! an...  not spam     0   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  Hi JamesHave you claim your complimentary gift...   \n",
       "1  alttextCongratulations you just earnednumberYo...   \n",
       "2  Heres your GitHub launch code Mortyjnumber an ...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [Hi, JamesHave, you, claim, your, complimentar...   \n",
       "1  [alttextCongratulations, you, just, earnednumb...   \n",
       "2  [Heres, your, GitHub, launch, code, Mortyjnumb...   \n",
       "\n",
       "                                        final_tokens  \n",
       "0  [hi, jameshav, claim, complimentari, gift, yet...  \n",
       "1  [alttextcongratul, earnednumbery, complet, fol...  \n",
       "2  [here, github, launch, code, mortyjnumb, octoc...  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df[\"cleaned_tokens\"].values:\n",
    "    for word in text:\n",
    "        cnt[word] += 1\n",
    " \n",
    "n_rare_words = 10\n",
    " \n",
    "Rare_words = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "def remove_rarewords(text):\n",
    "    \"\"\"custom function to remove the rare words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in Rare_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1['final_tokens']=df1['final_tokens'].apply(lambda x: remove_rarewords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['hi', 'jameshav', 'claim', 'complimentari', '...\n",
       "1    ['alttextcongratul', 'earnednumberi', 'complet...\n",
       "2    ['github', 'launch', 'code', 'mortyjnumb', 'oc...\n",
       "3    ['hello', 'thank', 'contact', 'virtual', 'rewa...\n",
       "4    ['hey', 'prachanda', 'rawaltoday', 'newslett',...\n",
       "Name: final_tokens, dtype: object"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['final_tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df1 is your DataFrame with 'final_tokens' column\n",
    "# If the final_tokens column already contains lists, skip the eval() part\n",
    "# Ensure the values in 'final_tokens' column are lists\n",
    "# If they are not, convert them to lists, otherwise skip this step\n",
    "\n",
    "# Get a list of all unique tokens\n",
    "all_tokens = list(set(token for sublist in df1['final_tokens'] for token in sublist))\n",
    "\n",
    "# Create a DataFrame where each unique token is a column\n",
    "token_df = pd.DataFrame(0, index=df1.index, columns=all_tokens)\n",
    "\n",
    "# Populate the DataFrame with token counts\n",
    "for i, tokens in enumerate(df1['final_tokens']):\n",
    "    for token in tokens:\n",
    "        token_df.at[i, token] += 1\n",
    "\n",
    "# Now, token_df contains the counts of each token in each row\n",
    "# You can concatenate it back to the original DataFrame if needed\n",
    "df1 = pd.concat([df1, token_df], axis=1)\n",
    "\n",
    "# Perform classification after this process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>spam</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>final_tokens</th>\n",
       "      <th>joined_text</th>\n",
       "      <th>mastercard</th>\n",
       "      <th>entertainmentwatch</th>\n",
       "      <th>...</th>\n",
       "      <th>multilingu</th>\n",
       "      <th>english</th>\n",
       "      <th>roger</th>\n",
       "      <th>employ</th>\n",
       "      <th>inact</th>\n",
       "      <th>playstat</th>\n",
       "      <th>accountwhen</th>\n",
       "      <th>jose</th>\n",
       "      <th>lloyd</th>\n",
       "      <th>everi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,Have you claim your complimentary gif...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>Hi JamesHave you claim your complimentary gift...</td>\n",
       "      <td>[Hi, JamesHave, you, claim, your, complimentar...</td>\n",
       "      <td>[hi, jameshav, claim, complimentari, gift, yet...</td>\n",
       "      <td>[ ' h i ' ,   ' j a m e s h a v ' ,   ' c l a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>alt_textCongratulations, you just earned500You...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>0</td>\n",
       "      <td>alttextCongratulations you just earnednumberYo...</td>\n",
       "      <td>[alttextCongratulations, you, just, earnednumb...</td>\n",
       "      <td>[alttextcongratul, earnednumberi, complet, fol...</td>\n",
       "      <td>[ ' a l t t e x t c o n g r a t u l ' ,   ' e ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420! an...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>0</td>\n",
       "      <td>Heres your GitHub launch code Mortyjnumber an ...</td>\n",
       "      <td>[Heres, your, GitHub, launch, code, Mortyjnumb...</td>\n",
       "      <td>[github, launch, code, mortyjnumb, octocat, st...</td>\n",
       "      <td>[ ' g i t h u b ' ,   ' l a u n c h ' ,   ' c ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello, Thank you for contacting the Virtual Re...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>0</td>\n",
       "      <td>Hello Thank you for contacting the Virtual Rew...</td>\n",
       "      <td>[Hello, Thank, you, for, contacting, the, Virt...</td>\n",
       "      <td>[hello, thank, contact, virtual, reward, cente...</td>\n",
       "      <td>[ ' h e l l o ' ,   ' t h a n k ' ,   ' c o n ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,Today's newsletter is Jam-...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey Prachanda RawalTodays newsletter is JamPac...</td>\n",
       "      <td>[Hey, Prachanda, RawalTodays, newsletter, is, ...</td>\n",
       "      <td>[hey, prachanda, rawaltoday, newslett, jampack...</td>\n",
       "      <td>[ ' h e y ' ,   ' p r a c h a n d a ' ,   ' r ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2490 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  spam  \\\n",
       "0  Hi James,Have you claim your complimentary gif...      spam     1   \n",
       "1  alt_textCongratulations, you just earned500You...  not spam     0   \n",
       "2  Here's your GitHub launch code, @Mortyj420! an...  not spam     0   \n",
       "3  Hello, Thank you for contacting the Virtual Re...  not spam     0   \n",
       "4  Hey Prachanda Rawal,Today's newsletter is Jam-...      spam     1   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  Hi JamesHave you claim your complimentary gift...   \n",
       "1  alttextCongratulations you just earnednumberYo...   \n",
       "2  Heres your GitHub launch code Mortyjnumber an ...   \n",
       "3  Hello Thank you for contacting the Virtual Rew...   \n",
       "4  Hey Prachanda RawalTodays newsletter is JamPac...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [Hi, JamesHave, you, claim, your, complimentar...   \n",
       "1  [alttextCongratulations, you, just, earnednumb...   \n",
       "2  [Heres, your, GitHub, launch, code, Mortyjnumb...   \n",
       "3  [Hello, Thank, you, for, contacting, the, Virt...   \n",
       "4  [Hey, Prachanda, RawalTodays, newsletter, is, ...   \n",
       "\n",
       "                                        final_tokens  \\\n",
       "0  [hi, jameshav, claim, complimentari, gift, yet...   \n",
       "1  [alttextcongratul, earnednumberi, complet, fol...   \n",
       "2  [github, launch, code, mortyjnumb, octocat, st...   \n",
       "3  [hello, thank, contact, virtual, reward, cente...   \n",
       "4  [hey, prachanda, rawaltoday, newslett, jampack...   \n",
       "\n",
       "                                         joined_text  mastercard  \\\n",
       "0  [ ' h i ' ,   ' j a m e s h a v ' ,   ' c l a ...           0   \n",
       "1  [ ' a l t t e x t c o n g r a t u l ' ,   ' e ...           0   \n",
       "2  [ ' g i t h u b ' ,   ' l a u n c h ' ,   ' c ...           0   \n",
       "3  [ ' h e l l o ' ,   ' t h a n k ' ,   ' c o n ...           0   \n",
       "4  [ ' h e y ' ,   ' p r a c h a n d a ' ,   ' r ...           0   \n",
       "\n",
       "   entertainmentwatch  ...  multilingu  english  roger  employ  inact  \\\n",
       "0                   0  ...           0        0      0       0      0   \n",
       "1                   0  ...           0        0      0       0      0   \n",
       "2                   0  ...           0        0      0       0      0   \n",
       "3                   0  ...           0        0      0       0      0   \n",
       "4                   0  ...           0        0      1       0      0   \n",
       "\n",
       "   playstat  accountwhen  jose  lloyd  everi  \n",
       "0         0            0     0      0      0  \n",
       "1         0            0     0      0      0  \n",
       "2         0            0     0      0      0  \n",
       "3         0            0     0      0      0  \n",
       "4         0            0     0      0      1  \n",
       "\n",
       "[5 rows x 2490 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[209], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Step 2: Create a document-term matrix using TfidfVectorizer (or use CountVectorizer for raw counts)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()  \u001b[38;5;66;03m# You can use CountVectorizer if you prefer\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoined_text\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Fit and transform the text data into features\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Step 3: Define the target variable (label)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m y \u001b[38;5;241m=\u001b[39m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspam\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Replace 'label_column' with the actual column name of your labels\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kelvi\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2138\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2133\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2134\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2135\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2136\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2137\u001b[0m )\n\u001b[1;32m-> 2138\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kelvi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kelvi\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kelvi\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1295\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1296\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1297\u001b[0m         )\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Assuming df1 is your dataframe and 'final_tokens' is the column with processed text\n",
    "# 'label_column' is the column containing the labels for classification (e.g., spam/ham, positive/negative)\n",
    "\n",
    "# Step 1: Join the tokenized text back into a single string (for vectorization)\n",
    "df1['joined_text'] = df1['final_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Step 2: Create a document-term matrix using TfidfVectorizer (or use CountVectorizer for raw counts)\n",
    "vectorizer = TfidfVectorizer()  # You can use CountVectorizer if you prefer\n",
    "X = vectorizer.fit_transform(df1['joined_text'])  # Fit and transform the text data into features\n",
    "\n",
    "# Step 3: Define the target variable (label)\n",
    "y = df1['spam']  # Replace 'label_column' with the actual column name of your labels\n",
    "\n",
    "# Step 4: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train the Naive Bayes model\n",
    "nb_model = MultinomialNB()  # Instantiate the Naive Bayes model\n",
    "nb_model.fit(X_train, y_train)  # Fit the model with the training data\n",
    "\n",
    "# Step 6: Make predictions on the test data\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      access   account   address      also  amazon  amazoncom  answered  \\\n",
      "0   0.000000  0.000000  0.000000  0.000000     0.0        0.0       0.0   \n",
      "1   0.000000  0.000000  0.000000  0.000000     0.0        0.0       0.0   \n",
      "2   0.000000  0.000000  0.000000  0.000000     0.0        0.0       0.0   \n",
      "3   0.000000  0.000000  0.000000  0.000000     0.0        0.0       0.0   \n",
      "4   0.000000  0.000000  0.000000  0.034238     0.0        0.0       0.0   \n",
      "..       ...       ...       ...       ...     ...        ...       ...   \n",
      "79  0.000000  0.000000  0.000000  0.000000     0.0        0.0       0.0   \n",
      "80  0.000000  0.000000  0.000000  0.000000     0.0        0.0       0.0   \n",
      "81  0.224312  0.645907  0.000000  0.105221     0.0        0.0       0.0   \n",
      "82  0.046986  0.115967  0.352643  0.000000     0.0        0.0       0.0   \n",
      "83  0.000000  0.000000  0.000000  0.000000     0.0        0.0       0.0   \n",
      "\n",
      "         app  application      best  ...  upvote        us       use  using  \\\n",
      "0   0.000000     0.000000  0.000000  ...     0.0  0.000000  0.000000    0.0   \n",
      "1   0.000000     0.000000  0.000000  ...     0.0  0.000000  0.000000    0.0   \n",
      "2   0.000000     0.000000  0.000000  ...     0.0  0.000000  0.000000    0.0   \n",
      "3   0.000000     0.000000  0.000000  ...     0.0  0.000000  0.000000    0.0   \n",
      "4   0.000000     0.000000  0.034238  ...     0.0  0.026444  0.032360    0.0   \n",
      "..       ...          ...       ...  ...     ...       ...       ...    ...   \n",
      "79  0.000000     0.665494  0.000000  ...     0.0  0.000000  0.000000    0.0   \n",
      "80  0.000000     0.000000  0.349047  ...     0.0  0.000000  0.000000    0.0   \n",
      "81  0.000000     0.000000  0.000000  ...     0.0  0.081267  0.000000    0.0   \n",
      "82  0.136381     0.000000  0.000000  ...     0.0  0.068091  0.124985    0.0   \n",
      "83  0.000000     0.000000  0.000000  ...     0.0  0.000000  0.000000    0.0   \n",
      "\n",
      "       visit      well      work  working     would      year  \n",
      "0   0.000000  0.000000  0.000000      0.0  0.000000  0.000000  \n",
      "1   0.469728  0.000000  0.000000      0.0  0.000000  0.000000  \n",
      "2   0.000000  0.000000  0.000000      0.0  0.000000  0.000000  \n",
      "3   0.000000  0.000000  0.000000      0.0  0.000000  0.000000  \n",
      "4   0.000000  0.039321  0.036495      0.0  0.000000  0.039321  \n",
      "..       ...       ...       ...      ...       ...       ...  \n",
      "79  0.000000  0.000000  0.000000      0.0  0.281724  0.000000  \n",
      "80  0.000000  0.000000  0.000000      0.0  0.000000  0.000000  \n",
      "81  0.000000  0.120841  0.000000      0.0  0.000000  0.000000  \n",
      "82  0.000000  0.000000  0.000000      0.0  0.000000  0.000000  \n",
      "83  0.241745  0.000000  0.000000      0.0  0.000000  0.000000  \n",
      "\n",
      "[84 rows x 100 columns]\n",
      "   access  account  address      also  amazon  amazoncom  answered  app  \\\n",
      "0     0.0      0.0      0.0  0.000000     0.0        0.0       0.0  0.0   \n",
      "1     0.0      0.0      0.0  0.000000     0.0        0.0       0.0  0.0   \n",
      "2     0.0      0.0      0.0  0.000000     0.0        0.0       0.0  0.0   \n",
      "3     0.0      0.0      0.0  0.000000     0.0        0.0       0.0  0.0   \n",
      "4     0.0      0.0      0.0  0.034238     0.0        0.0       0.0  0.0   \n",
      "\n",
      "   application      best  ...  upvote        us      use  using     visit  \\\n",
      "0          0.0  0.000000  ...     0.0  0.000000  0.00000    0.0  0.000000   \n",
      "1          0.0  0.000000  ...     0.0  0.000000  0.00000    0.0  0.469728   \n",
      "2          0.0  0.000000  ...     0.0  0.000000  0.00000    0.0  0.000000   \n",
      "3          0.0  0.000000  ...     0.0  0.000000  0.00000    0.0  0.000000   \n",
      "4          0.0  0.034238  ...     0.0  0.026444  0.03236    0.0  0.000000   \n",
      "\n",
      "       well      work  working  would      year  \n",
      "0  0.000000  0.000000      0.0    0.0  0.000000  \n",
      "1  0.000000  0.000000      0.0    0.0  0.000000  \n",
      "2  0.000000  0.000000      0.0    0.0  0.000000  \n",
      "3  0.000000  0.000000      0.0    0.0  0.000000  \n",
      "4  0.039321  0.036495      0.0    0.0  0.039321  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming df1['joined_text'] contains the joined tokenized text\n",
    "\n",
    "# Step 1: Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(max_features=100)  # You can use CountVectorizer if preferred\n",
    "X = vectorizer.fit_transform(df1['joined_text'])  # This is a sparse matrix\n",
    "\n",
    "# Step 2: Convert the sparse matrix to a DataFrame\n",
    "# Get the feature names (terms/words) from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert sparse matrix to dense (array format) and then to DataFrame\n",
    "X_dense = X.toarray()  # Converts sparse matrix to a dense NumPy array\n",
    "df_matrix = pd.DataFrame(X_dense, columns=feature_names)\n",
    "\n",
    "# Step 3: Print or display the document-term matrix\n",
    "print(df_matrix)\n",
    "\n",
    "# If it's too large, you can display only the first few rows\n",
    "print(df_matrix.head())  # Displays the first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
